"""
üèÜ CAFA-6: ESM2 + PubMedBERT Text Mining
========================================
Combines:
1. ESM2 embeddings (sequence/structure)
2. PubMedBERT embeddings (text descriptions)

REQUIRED DATASETS:
- cafa-6-protein-function-prediction
- Add one of these for UniProt descriptions:
  - "uniprot-description-only" 
  - OR "uniprot-sprot"
  - OR fetch via API (slower)

RUN WITH: GPU T4 x2
TIME: ~8-10 hours
"""

!pip install transformers biopython obonet requests -q

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModel
from Bio import SeqIO
import obonet
from pathlib import Path
from datetime import datetime
from sklearn.model_selection import StratifiedKFold
import gc
import os
import requests
from concurrent.futures import ThreadPoolExecutor
import time

print("="*80)
print("üèÜ CAFA-6: ESM2 + PubMedBERT Text Mining")
print("="*80)

n_gpus = torch.cuda.device_count()
print(f"GPUs: {n_gpus}")
device = torch.device("cuda:0")

# ============================================================================
# CONFIG
# ============================================================================

CONFIG = {
    # Model
    'hidden_dims': [1536, 768, 384, 192],  # Larger for combined embeddings
    'dropout': 0.4,
    'residual': True,
    'hierarchy_weight': 0.1,
    
    # Training
    'lr': 5e-4,
    'epochs': 20,
    'batch_size': 128,
    'n_folds': 5,
    'n_terms': 1000,
    
    # Models
    'esm_model': "facebook/esm2_t33_650M_UR50D",
    'text_model': "microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract",
    'max_seq_len': 1022,
    'max_text_len': 256,
}

# ============================================================================
# LOAD DATA
# ============================================================================

print("\n[1/7] Loading CAFA-6 data...")
CAFA6_DIR = Path("/kaggle/input/cafa-6-protein-function-prediction/")

# Load proteins
train_proteins = {}
for record in SeqIO.parse(CAFA6_DIR / "Train/train_sequences.fasta", "fasta"):
    acc = record.id.split('|')[1] if '|' in record.id else record.id
    train_proteins[acc] = str(record.seq)

test_proteins = {}
for record in SeqIO.parse(CAFA6_DIR / "Test/testsuperset.fasta", "fasta"):
    acc = record.id.split('|')[1] if '|' in record.id else record.id
    test_proteins[acc] = str(record.seq)

print(f"  Train: {len(train_proteins):,}, Test: {len(test_proteins):,}")

# Load labels
df_terms = pd.read_csv(CAFA6_DIR / "Train/train_terms.tsv", sep='\t', names=['EntryID', 'term', 'aspect'])
df_terms = df_terms[df_terms['EntryID'] != 'EntryID']
protein_to_terms = df_terms.groupby('EntryID')['term'].apply(set).to_dict()

# Load GO hierarchy
go_graph = obonet.read_obo(str(CAFA6_DIR / "Train/go-basic.obo"))

def get_ancestors(term, graph):
    ancestors = set()
    if term in graph:
        for parent in graph.predecessors(term):
            ancestors.add(parent)
            ancestors.update(get_ancestors(parent, graph))
    return ancestors

# ============================================================================
# LOAD UNIPROT DESCRIPTIONS
# ============================================================================

print("\n[2/7] Loading UniProt descriptions...")

# Try to load from dataset
uniprot_desc = {}

# Check for uniprot-description-only dataset
desc_paths = [
    "/kaggle/input/uniprot-description-only/",
    "/kaggle/input/uniprot-sprot/",
    "/kaggle/input/swissprot/",
]

for path in desc_paths:
    if os.path.exists(path):
        print(f"  Found dataset at: {path}")
        for f in os.listdir(path):
            if f.endswith('.csv') or f.endswith('.tsv'):
                try:
                    df = pd.read_csv(os.path.join(path, f), sep='\t' if f.endswith('.tsv') else ',')
                    # Try common column names
                    for acc_col in ['Entry', 'accession', 'id', 'protein_id']:
                        for desc_col in ['Protein names', 'description', 'Function [CC]', 'protein_name']:
                            if acc_col in df.columns and desc_col in df.columns:
                                for _, row in df.iterrows():
                                    uniprot_desc[row[acc_col]] = str(row[desc_col])[:500]
                                print(f"    Loaded {len(uniprot_desc):,} descriptions")
                                break
                except:
                    pass
        break

# If no dataset found, fetch via API (slower but works)
if len(uniprot_desc) == 0:
    print("  No local dataset found. Fetching via UniProt API...")
    
    all_accs = list(set(list(train_proteins.keys()) + list(test_proteins.keys())))
    
    def fetch_uniprot_batch(accs):
        """Fetch descriptions for a batch of accessions."""
        results = {}
        try:
            acc_str = ' OR '.join(accs[:100])  # API limit
            url = f"https://rest.uniprot.org/uniprotkb/search?query={acc_str}&fields=accession,protein_name&format=tsv"
            response = requests.get(url, timeout=30)
            if response.status_code == 200:
                lines = response.text.strip().split('\n')
                for line in lines[1:]:  # Skip header
                    parts = line.split('\t')
                    if len(parts) >= 2:
                        results[parts[0]] = parts[1][:500]
        except Exception as e:
            pass
        return results
    
    # Fetch in batches
    batch_size = 50
    for i in range(0, min(len(all_accs), 50000), batch_size):  # Limit to 50K for time
        batch = all_accs[i:i+batch_size]
        results = fetch_uniprot_batch(batch)
        uniprot_desc.update(results)
        if i % 1000 == 0:
            print(f"    Fetched {len(uniprot_desc):,} descriptions...")
        time.sleep(0.1)  # Rate limiting

print(f"  Total descriptions: {len(uniprot_desc):,}")

# Create description for each protein
# Augment train descriptions with known GO terms
train_descriptions = {}
for acc in train_proteins:
    base_desc = uniprot_desc.get(acc, f"Protein {acc}")
    # Add known GO terms to description for richer context
    if acc in protein_to_terms:
        # Get term names from GO graph
        term_names = []
        for term_id in list(protein_to_terms[acc])[:10]:  # Limit to 10 terms
            if term_id in go_graph:
                term_names.append(go_graph.nodes[term_id].get('name', term_id))
        if term_names:
            base_desc += " [GO terms: " + ", ".join(term_names) + "]"
    train_descriptions[acc] = base_desc

test_descriptions = {acc: uniprot_desc.get(acc, f"Protein {acc}") for acc in test_proteins}

# ============================================================================
# GENERATE EMBEDDINGS
# ============================================================================

print("\n[3/7] Generating ESM2 embeddings...")

esm_tokenizer = AutoTokenizer.from_pretrained(CONFIG['esm_model'])
esm_model = AutoModel.from_pretrained(CONFIG['esm_model']).to(device).eval()

def get_esm_embeddings(proteins, batch_size=4):
    accs = list(proteins.keys())
    seqs = [proteins[acc][:CONFIG['max_seq_len']] for acc in accs]
    embeddings = []
    
    for i in range(0, len(seqs), batch_size):
        batch = seqs[i:i+batch_size]
        inputs = esm_tokenizer(batch, return_tensors="pt", padding=True, truncation=True, max_length=CONFIG['max_seq_len']+2)
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = esm_model(**inputs)
            emb = outputs.last_hidden_state.mean(dim=1)
            embeddings.append(emb.cpu().numpy())
        
        if (i // batch_size) % 200 == 0:
            print(f"    {i+len(batch)}/{len(seqs)}")
            torch.cuda.empty_cache()
    
    return accs, np.vstack(embeddings)

train_accs, X_train_esm = get_esm_embeddings(train_proteins)
print(f"  Train ESM2: {X_train_esm.shape}")

test_accs, X_test_esm = get_esm_embeddings(test_proteins)
print(f"  Test ESM2: {X_test_esm.shape}")

del esm_model
gc.collect()
torch.cuda.empty_cache()

print("\n[4/7] Generating PubMedBERT text embeddings...")

text_tokenizer = AutoTokenizer.from_pretrained(CONFIG['text_model'])
text_model = AutoModel.from_pretrained(CONFIG['text_model']).to(device).eval()

def get_text_embeddings(descriptions, batch_size=16):
    accs = list(descriptions.keys())
    texts = [descriptions[acc] for acc in accs]
    embeddings = []
    
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        inputs = text_tokenizer(batch, return_tensors="pt", padding=True, truncation=True, max_length=CONFIG['max_text_len'])
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = text_model(**inputs)
            emb = outputs.last_hidden_state[:, 0, :]  # CLS token
            embeddings.append(emb.cpu().numpy())
        
        if (i // batch_size) % 500 == 0:
            print(f"    {i+len(batch)}/{len(texts)}")
            torch.cuda.empty_cache()
    
    return np.vstack(embeddings)

X_train_text = get_text_embeddings(train_descriptions)
print(f"  Train text: {X_train_text.shape}")

X_test_text = get_text_embeddings(test_descriptions)
print(f"  Test text: {X_test_text.shape}")

del text_model
gc.collect()
torch.cuda.empty_cache()

print("\n[5/7] Combining embeddings...")
# Concatenate ESM2 + PubMedBERT
X_train = np.concatenate([X_train_esm, X_train_text], axis=1)
X_test = np.concatenate([X_test_esm, X_test_text], axis=1)
print(f"  Combined: Train {X_train.shape}, Test {X_test.shape}")

# ============================================================================
# MODEL
# ============================================================================

class ResidualBlock(nn.Module):
    def __init__(self, dim, dropout):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(dim, dim), nn.BatchNorm1d(dim), nn.GELU(), nn.Dropout(dropout),
            nn.Linear(dim, dim), nn.BatchNorm1d(dim)
        )
        self.act = nn.GELU()
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x):
        return self.dropout(self.act(x + self.net(x)))

class MultiModalModel(nn.Module):
    """Combines sequence and text embeddings."""
    def __init__(self, input_dim, hidden_dims, n_terms, dropout):
        super().__init__()
        
        self.input_proj = nn.Sequential(
            nn.Linear(input_dim, hidden_dims[0]),
            nn.BatchNorm1d(hidden_dims[0]),
            nn.GELU(),
            nn.Dropout(dropout),
        )
        
        layers = []
        for i in range(len(hidden_dims) - 1):
            layers.append(nn.Sequential(
                nn.Linear(hidden_dims[i], hidden_dims[i+1]),
                nn.BatchNorm1d(hidden_dims[i+1]),
                nn.GELU(),
                nn.Dropout(dropout),
            ))
        self.backbone = nn.Sequential(*layers)
        
        self.head = nn.Sequential(
            nn.Linear(hidden_dims[-1], n_terms),
            nn.Sigmoid()
        )
        
    def forward(self, x):
        x = self.input_proj(x)
        x = self.backbone(x)
        return self.head(x)

class HierarchicalLoss(nn.Module):
    def __init__(self, parent_child_pairs, weight=0.1):
        super().__init__()
        self.parent_child_pairs = parent_child_pairs
        self.weight = weight
        self.bce = nn.BCELoss()
    
    def forward(self, pred, target):
        bce_loss = self.bce(pred, target)
        if len(self.parent_child_pairs) > 0:
            hierarchy_loss = sum(
                F.relu(pred[:, c] - pred[:, p]).mean() 
                for p, c in self.parent_child_pairs
            ) / len(self.parent_child_pairs)
            return bce_loss + self.weight * hierarchy_loss
        return bce_loss

# ============================================================================
# TRAINING
# ============================================================================

print("\n[6/7] Training multi-modal model...")

train_acc_to_idx = {acc: i for i, acc in enumerate(train_accs)}
all_predictions = []

for aspect in ['F', 'P', 'C']:
    print(f"\n{'='*60}")
    print(f"ASPECT: {aspect}")
    print(f"{'='*60}")
    
    aspect_df = df_terms[df_terms['aspect'] == aspect]
    terms = aspect_df['term'].value_counts().head(CONFIG['n_terms']).index.tolist()
    term_to_idx = {t: i for i, t in enumerate(terms)}
    n_terms = len(terms)
    
    parent_child_pairs = []
    for i, term in enumerate(terms):
        for anc in get_ancestors(term, go_graph):
            if anc in term_to_idx:
                parent_child_pairs.append((term_to_idx[anc], i))
    
    y_train = np.zeros((len(train_accs), n_terms), dtype=np.float32)
    for acc, prot_terms in protein_to_terms.items():
        if acc in train_acc_to_idx:
            idx = train_acc_to_idx[acc]
            for j, term in enumerate(terms):
                if term in prot_terms:
                    y_train[idx, j] = 1
    
    print(f"  Terms: {n_terms}, Positive: {int(y_train.sum()):,}")
    
    aspect_preds = np.zeros((len(test_accs), n_terms), dtype=np.float32)
    kfold = StratifiedKFold(n_splits=CONFIG['n_folds'], shuffle=True, random_state=42)
    strat = (y_train.sum(axis=1) > 0).astype(int)
    
    for fold, (tr_idx, val_idx) in enumerate(kfold.split(X_train, strat)):
        print(f"\n  Fold {fold+1}/{CONFIG['n_folds']}")
        
        model = MultiModalModel(X_train.shape[1], CONFIG['hidden_dims'], n_terms, CONFIG['dropout'])
        if n_gpus >= 2:
            model = nn.DataParallel(model)
        model = model.to(device)
        
        optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['lr'], weight_decay=0.01)
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, CONFIG['epochs'])
        criterion = HierarchicalLoss(parent_child_pairs, CONFIG['hierarchy_weight'])
        
        train_loader = DataLoader(
            torch.utils.data.TensorDataset(
                torch.tensor(X_train[tr_idx], dtype=torch.float32),
                torch.tensor(y_train[tr_idx], dtype=torch.float32)
            ),
            batch_size=CONFIG['batch_size'] * n_gpus, shuffle=True
        )
        
        model.train()
        for epoch in range(CONFIG['epochs']):
            total_loss = 0
            for bx, by in train_loader:
                bx, by = bx.to(device), by.to(device)
                optimizer.zero_grad()
                outputs = model(bx)
                loss = criterion(outputs, by)
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                optimizer.step()
                total_loss += loss.item()
            scheduler.step()
            
            if (epoch + 1) % 5 == 0:
                print(f"    Epoch {epoch+1}: Loss = {total_loss/len(train_loader):.4f}")
        
        model.eval()
        test_loader = DataLoader(torch.tensor(X_test, dtype=torch.float32), batch_size=CONFIG['batch_size'] * n_gpus)
        fold_preds = []
        with torch.no_grad():
            for bx in test_loader:
                bx = bx.to(device)
                fold_preds.append(model(bx).cpu().numpy())
        
        aspect_preds += np.vstack(fold_preds)
        del model
        gc.collect()
        torch.cuda.empty_cache()
    
    aspect_preds /= CONFIG['n_folds']
    
    for i, acc in enumerate(test_accs):
        for j, term in enumerate(terms):
            score = float(aspect_preds[i, j])
            if score > 0.01:
                all_predictions.append({'protein': acc, 'term': term, 'score': round(score, 6)})

# ============================================================================
# SUBMISSION
# ============================================================================

print("\n[7/7] Creating submission...")
df_preds = pd.DataFrame(all_predictions)
print(f"  Total: {len(df_preds):,}")

df_preds.to_csv('/kaggle/working/submission.tsv', sep='\t', index=False, header=False)

size_mb = os.path.getsize('/kaggle/working/submission.tsv') / 1024 / 1024
print(f"  Size: {size_mb:.1f} MB")

print("\n" + "="*80)
print("üèÜ ESM2 + PubMedBERT SUBMISSION READY!")
print("="*80)
print(f"Expected F-max: 0.40-0.50 (text mining boost!)")
